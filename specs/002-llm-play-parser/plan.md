# Implementation Plan: LLM-Powered Play Parser

**Branch**: `002-llm-play-parser` | **Date**: 2025-12-01 | **Spec**: [spec.md](spec.md)
**Input**: Feature specification from `/specs/002-llm-play-parser/spec.md`

**Note**: This template is filled in by the `/speckit.plan` command. See `.specify/templates/commands/plan.md` for the execution workflow.

## Summary

This feature enables users to upload play scripts (PDF, DOCX, TXT) and have them parsed automatically using an LLM to extract structured play data conforming to the StructuredPlay model. The parser streams real-time progress updates and produces validated data including title, author, characters, acts, scenes, and line-by-line dialogue with character attribution and stage directions. The system uses Vercel AI SDK with structured output (generateObject/streamObject) and Zod schemas for type-safe validation. Critically, all entity IDs are generated by the database using `@default(uuid())` in Prisma, while LLM-generated IDs serve only as temporary references during parsing and are stored as supplementary `llmSourceId` fields for debugging.

## Technical Context

**Language/Version**: TypeScript with Next.js 15 (App Router)  
**Primary Dependencies**: Next.js, Prisma (PostgreSQL), Vercel AI SDK, Zod, pdf-parse, mammoth  
**Storage**: PostgreSQL (existing schema extended with llmSourceId fields)  
**Testing**: Jest with contract tests for Zod schemas, integration tests for LLM parsing  
**Target Platform**: Web (Next.js full-stack with API routes)  
**Project Type**: Next.js full-stack (single app with client + server)  
**Performance Goals**: File parsing within 3 minutes for standard plays (2-3 acts), real-time SSE streaming updates every 5 seconds  
**Constraints**: LLM API compatible with Vercel AI SDK (Anthropic preferred, OpenAI fallback), structured output validation via Zod, database-generated IDs only  
**Scale/Scope**: Individual file uploads up to 500 pages/5MB, support for PDF/DOCX/TXT formats

## Constitution Check

_GATE: Must pass before Phase 0 research. Re-check after Phase 1 design._

Verify compliance with Theatre Play Buddy Constitution principles:

- [x] **Principle I (Actor-Centered Practice)**: N/A - This is an ingestion feature supporting the practice flow by enabling play imports
- [x] **Principle II (Multi-Format Ingestion)**: ✅ FULL COMPLIANCE - Supports PDF, DOCX, and TXT formats
- [x] **Principle III (Structured Play Model)**: ✅ FULL COMPLIANCE - All parsed data validated against StructuredPlay model via Zod schemas; Prisma schema extended to support llmSourceId fields for debugging while maintaining database-generated canonical IDs
- [x] **Principle IV (Search & Discovery)**: ✅ ENABLED - Parsed plays include all metadata required for search (title, author, characters, scenes)
- [ ] **Principle V (Real-Time Audio)**: N/A - This feature does not involve audio processing
- [ ] **Principle VI (Error Assistance)**: N/A - Error handling is for parsing failures, not actor divergence
- [x] **Principle VII (Progress Tracking)**: ✅ ENABLED - Imported plays integrate with existing progress tracking via database schema
- [x] **Principle VIII (Privacy-First)**: ✅ COMPLIANCE WITH EXCEPTION - All parsed data stored locally in PostgreSQL; EXCEPTION: LLM API calls transmit play text to external service (Anthropic/OpenAI) for parsing - user must accept this during upload
- [x] **Performance Target**: ✅ COMPLIANT - <3 minutes for standard play parsing; SSE updates every 5 seconds; does not impact <200ms audio latency requirement
- [x] **Data Model Conformance**: ✅ FULL COMPLIANCE - Zod schemas mirror StructuredPlay model; database schema uses Prisma with canonical ID generation via @default(uuid())

**Privacy Exception Note**: While the constitution requires local-first operation, this feature necessarily sends play text to external LLM APIs for parsing. This is disclosed to users during upload and is essential for the automated parsing capability. Alternative approaches (local LLM) were considered but rejected due to accuracy/performance constraints.

## Project Structure

### Documentation (this feature)

```text
specs/002-llm-play-parser/
├── plan.md              # This file (filled by /speckit.plan command)
├── research.md          # Phase 0 output - COMPLETE
├── data-model.md        # Phase 1 output - COMPLETE (includes ID management strategy)
├── quickstart.md        # Phase 1 output - COMPLETE
├── contracts/           # Phase 1 output - COMPLETE
│   ├── upload-api.yaml  # File upload endpoint contract
│   └── parse-api.yaml   # SSE parsing endpoint contract
└── tasks.md             # Phase 2 output - COMPLETE (47 tasks defined)
```

### Source Code (repository root)

```text
# Next.js full-stack architecture (client + server in one project)
src/
├── app/
│   ├── import/
│   │   ├── page.tsx                    # Import UI page
│   │   └── api/
│   │       ├── upload/
│   │       │   └── route.ts            # POST /api/import/upload - File upload handler
│   │       └── parse/
│   │           └── route.ts            # POST /api/import/parse - SSE parsing endpoint
│   └── api/
│       └── plays/                      # Existing play API routes
│
├── components/
│   └── import/
│       ├── file-upload.tsx             # Drag-drop file upload component
│       ├── parse-progress.tsx          # SSE progress display
│       └── parse-error-display.tsx     # Error handling UI
│
├── lib/
│   ├── parse/
│   │   ├── schemas.ts                  # Zod schemas (PlaybookSchema, CharacterSchema, etc.)
│   │   ├── validation.ts               # Schema validation utilities
│   │   ├── extractors.ts               # PDF/DOCX/TXT text extraction
│   │   ├── llm-parser.ts               # LLM integration via Vercel AI SDK
│   │   ├── multi-character.ts          # Multi-speaker line helpers
│   │   └── errors.ts                   # Typed error responses
│   └── play-storage.ts                 # Extended with import/persistence helpers
│
prisma/
├── schema.prisma                       # Extended with llmSourceId fields on all models
└── migrations/                         # New migration for llmSourceId fields

tests/
├── parse/
│   ├── schemas.test.ts                 # Zod schema validation tests
│   ├── extractors.test.ts              # PDF/DOCX/TXT extraction tests
│   ├── multi-character.test.ts         # Multi-speaker attribution tests
│   └── llm-parser.test.ts              # Integration test: upload→parse→persist→render
└── contract/
    └── structured-play-model.test.ts   # StructuredPlay conformance tests
```

**Structure Decision**: Single Next.js full-stack project following existing architecture. All parsing logic in `src/lib/parse/`, API routes in `src/app/import/api/`, UI components in `src/components/import/`. Database schema extended in place with llmSourceId fields for ID mapping during import.

## Complexity Tracking

> **Fill ONLY if Constitution Check has violations that must be justified**

| Violation                            | Why Needed                                                                                                | Simpler Alternative Rejected Because                                                                                                                                                                                                      |
| ------------------------------------ | --------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| External LLM API (Privacy exception) | Automated parsing of unstructured play scripts requires advanced NLP capabilities beyond local processing | Local LLM options (e.g., Llama, local GPT variants) lack sufficient accuracy for reliable multi-format parsing and structured output generation. Manual entry alternative defeats purpose of feature (user uploads to avoid manual work). |
| Database ID mapping complexity       | LLM cannot generate database-compatible UUIDs that guarantee uniqueness and referential integrity         | Allowing LLM to generate primary keys would risk ID collisions, break foreign key constraints, and violate database authority principle. Mapping layer is essential to maintain data integrity.                                           |

**Justification**: The external LLM API call is essential for this feature's core value proposition (automated parsing). Users are explicitly informed during upload that the play text will be sent to the LLM provider. The alternative (manual play entry) already exists but is time-prohibitive for actors who want to practice with multiple plays. The ID mapping complexity is unavoidable to maintain database integrity while leveraging LLM structured output capabilities.

---

## Implementation Phases

### Phase 0: Research & Knowledge Gathering ✅ COMPLETE

**Objective**: Resolve all technical unknowns and establish best practices for LLM parsing, structured output, and multi-format text extraction.

**Outputs**:

- [research.md](research.md) - Completed research covering:
  - Vercel AI SDK structured output capabilities (generateObject/streamObject)
  - Zod schema design for StructuredPlay model validation
  - PDF/DOCX/TXT extraction libraries (pdf-parse, mammoth, fs.readFile)
  - Multi-character line attribution strategies
  - SSE streaming patterns for real-time progress updates
  - LLM provider selection (Anthropic Claude Sonnet preferred, OpenAI GPT-4 fallback)

**Key Decisions**:

- Use Vercel AI SDK for type-safe LLM integration
- Multi-stage parsing: metadata → structure → lines (prevents context window overflow)
- Database-generated IDs with LLM ID mapping via `Map<llmSourceId, dbId>`

### Phase 1: Design & Contracts ✅ COMPLETE

**Objective**: Define data schemas, API contracts, and development quickstart.

**Outputs**:

- [data-model.md](data-model.md) - Zod schemas for all entities with ID management strategy documented
- [contracts/upload-api.yaml](contracts/upload-api.yaml) - File upload endpoint specification
- [contracts/parse-api.yaml](contracts/parse-api.yaml) - SSE parsing endpoint with all event types
- [quickstart.md](quickstart.md) - Development environment setup and first-run guide

**Key Artifacts**:

- `PlaybookSchema`, `CharacterSchema`, `ActSchema`, `SceneSchema`, `LineSchema` with `llmSourceId` fields
- Line attribution supports single speaker (`characterLlmId`) and multi-speaker (`characterLlmIdArray`)
- SSE events: `progress`, `character_found`, `act_complete`, `scene_complete`, `complete`, `error`, `unsupported_speaker`

**Agent Context Update**: ✅ COMPLETE

- Updated `.github/copilot-instructions.md` with Zod schemas and Vercel AI SDK patterns

### Phase 2: Task Breakdown ✅ COMPLETE

**Objective**: Break down implementation into granular, testable tasks organized by user story priority.

**Outputs**:

- [tasks.md](tasks.md) - 47 tasks organized across 6 phases:
  - Setup (7 tasks): Environment, dependencies, directory structure
  - Foundational (7 tasks): Schemas, extractors, LLM parser, validation
  - User Story 1 - P1 (10 tasks): Upload + basic structure extraction
  - User Story 2 - P2 (6 tasks): Line-by-line character attribution
  - User Story 3 - P3 (5 tasks): Stage direction attribution
  - User Story 4 - P4 (3 tasks): Format preservation
  - Polish (9 tasks): Testing, error handling, telemetry

**Execution Strategy**:

- MVP: Story 1 (upload + basic structure) delivers immediate value
- Incremental: Stories 2-4 add progressive refinement
- Validation-first: All LLM outputs validated through Zod schemas
- Test coverage: Unit tests for extractors/schemas, integration test for full pipeline

### Phase 3: Implementation (IN PROGRESS)

**Current Status**: All foundational and User Story 1-4 tasks marked complete in tasks.md.

**Next Steps**:

1. Create database migration for `llmSourceId` fields on Playbook, Character, Act, Scene, Line models
2. Update Prisma client and verify schema changes
3. Run test suite to validate schema conformance
4. Deploy to local environment and test end-to-end upload→parse→persist flow

**Blockers**: None - All prerequisites complete

### Phase 4: Testing & Validation (PENDING)

**Test Coverage Requirements**:

- [x] Unit tests for Zod schemas (`tests/parse/schemas.test.ts`)
- [x] Unit tests for extractors (`tests/parse/extractors.test.ts`)
- [x] Unit tests for multi-character helpers (`tests/parse/multi-character.test.ts`)
- [x] Integration test for full pipeline (`tests/parse/llm-parser.test.ts`)
- [ ] E2E test: Upload real play → Parse → Verify in database → Render in UI
- [ ] Contract test: Verify parsed data conforms to StructuredPlay model

**Validation Criteria**:

- 95%+ character identification accuracy
- 90%+ stage direction categorization accuracy
- <3 minute parsing time for standard plays
- SSE updates every 5 seconds
- Zero schema validation failures on well-formed scripts

### Phase 5: Documentation & Deployment (PENDING)

**Documentation Updates**:

- [x] README updated with import feature quickstart
- [ ] User guide for file upload and parsing
- [ ] Troubleshooting guide for common parsing errors
- [ ] API documentation for upload/parse endpoints

**Deployment Checklist**:

- [ ] Environment variables configured (ANTHROPIC_API_KEY/OPENAI_API_KEY)
- [ ] Database migration applied
- [ ] LLM API keys validated
- [ ] File upload limits configured (5MB, 500 pages)
- [ ] Error monitoring enabled for parsing failures

---

## Risk Assessment

| Risk                                | Impact | Mitigation Strategy                                                             |
| ----------------------------------- | ------ | ------------------------------------------------------------------------------- |
| LLM parsing accuracy <95%           | High   | Multi-stage parsing with validation; retry logic; manual correction UI          |
| File size/complexity causes timeout | Medium | Chunked parsing; progress checkpoints; resume capability                        |
| LLM API rate limits/costs           | Medium | Implement rate limiting; cost monitoring; fallback to OpenAI if Anthropic fails |
| Character name inconsistencies      | Medium | Normalize names in validation layer; allow user to merge duplicates             |
| Database ID mapping bugs            | High   | Comprehensive unit tests for mapping logic; transaction rollback on failures    |
| llmSourceId uniqueness violations   | Low    | Non-unique index on llmSourceId; graceful handling of duplicates                |

---

## Success Metrics

**Feature Adoption**:

- 80% of new users upload at least one play within first session
- Average upload-to-ready time <3 minutes

**Quality Metrics**:

- Character identification accuracy ≥95%
- Stage direction categorization ≥90%
- Schema validation failure rate <1%

**Performance Metrics**:

- Parsing completion within 3 minutes for 90th percentile of uploads
- SSE update frequency ≥1 update per 5 seconds
- Zero database constraint violations from ID mapping

**User Experience**:

- Clear error messages for 100% of parsing failures
- Progress visibility throughout parsing process
- Successful retry rate ≥70% after initial parsing failure
